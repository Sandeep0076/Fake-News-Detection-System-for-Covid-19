{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "3Jan_latest_with_check_balances",
      "provenance": [],
      "collapsed_sections": [
        "w_t89SR2lFKy",
        "p3FjTxP5lNc3",
        "cInLjRhStUKA"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOl+SxDEhNMAjgPn0tB+DKZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sandeep0076/Fake-News-Detection-System-for-Covid-19/blob/main/Model%20Deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eRGPe2NFcK-"
      },
      "source": [
        "#  Features with predictions and f1 score\n",
        "I have not made UI yet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhxC5T8cjwxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897809ee-d075-49d9-d938-5a3cb80a919f"
      },
      "source": [
        "!pip install transformers~=3.4.0\n",
        "!pip install boilerpy3\n",
        "#!pip install transformers\n",
        "#pip install pytorch-lightning"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers~=3.4.0 in /usr/local/lib/python3.7/dist-packages (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers~=3.4.0) (1.19.5)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.7/dist-packages (from transformers~=3.4.0) (0.9.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers~=3.4.0) (3.0.12)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers~=3.4.0) (3.12.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers~=3.4.0) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers~=3.4.0) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers~=3.4.0) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers~=3.4.0) (0.1.95)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers~=3.4.0) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers~=3.4.0) (2019.12.20)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers~=3.4.0) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers~=3.4.0) (54.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers~=3.4.0) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers~=3.4.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers~=3.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers~=3.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers~=3.4.0) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers~=3.4.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers~=3.4.0) (1.0.1)\n",
            "Requirement already satisfied: boilerpy3 in /usr/local/lib/python3.7/dist-packages (1.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tS8627UrkCA",
        "outputId": "b2c28db0-6a57-400f-95c2-25815081a9c5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/Gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/Gdrive; to attempt to forcibly remount, call drive.mount(\"/content/Gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_t89SR2lFKy"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJlOuLW_jSQB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced31ea5-9894-4492-b471-d2b2c9f2fc9b"
      },
      "source": [
        "%%writefile lstm_bert_model.py\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/Gdrive\")\n",
        "file_location=\"/content/Gdrive/MyDrive/covid19_news_dataset.csv\"\n",
        "\"\"\"## Imports\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support as score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,Dataset,random_split,SubsetRandomSampler\n",
        "\n",
        "import transformers\n",
        "from transformers import  BertTokenizer, RobertaModel, BertModel, AdamW, AutoConfig,get_linear_schedule_with_warmup\n",
        "\n",
        "import time \n",
        "from datetime import  date\n",
        "import warnings\n",
        "import collections \n",
        "from operator import truediv\n",
        "from boilerpy3 import extractors\n",
        "from urllib.parse import quote,unquote,urlparse\n",
        "from urllib.request import urlopen\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "## Classes\n",
        "# Base bert class for training and classification of the model\n",
        "class Bert_TextClassification_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Bert_TextClassification_Model, self).__init__()\n",
        "        self.num_classes = NUM_CLASSES\n",
        "        self.bert_path = 'bert-base-uncased'\n",
        "        # Adding Gradient checkpoint for reducing memory usage\n",
        "        #it is saving 50% of memory in my case before the Model was going out of memory even with 11Gb now\n",
        "        #now after deleting unused variable, checkpointing and smaller variables consuption is reduced to 4Gb\n",
        "        self.config = AutoConfig.from_pretrained(self.bert_path,\n",
        "                                                 gradient_checkpointing=True, )\n",
        "        #for memory conservation by using smaller value\n",
        "        self.config.use_bfloat16 = True\n",
        "        self.bert = transformers.BertModel.from_pretrained(self.bert_path, config=self.config)\n",
        "        self.out = nn.Linear(768, self.num_classes)\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, pooled_out = self.bert(\n",
        "            ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "        return self.out(pooled_out)\n",
        "\n",
        "# This implements Bert take out the fined tuned model excluding last and add LSTM and linear layer on top of it to achieve higher accuracy\n",
        "class RoBERT_Model(nn.Module):\n",
        "\n",
        "    def __init__(self, bertFineTuned):\n",
        "        super(RoBERT_Model, self).__init__()\n",
        "        self.num_classes = NUM_CLASSES\n",
        "        # old model is initialized which is trained and remembers the weights and bais.\n",
        "        # since bert model only take 512 we will input the data with the same order\n",
        "        # but before we feed to the lstm layer,we will combine the output of bert which was previously\n",
        "        # divided into 200, seg and then in last filter out in linear\n",
        "        self.bertFineTuned = bertFineTuned\n",
        "        self.lstm = nn.LSTM(768, 100, num_layers=1, bidirectional=False)\n",
        "        self.out = nn.Linear(100, self.num_classes)\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids, num_chunks):\n",
        "        _, pooled_out = self.bertFineTuned(ids,\n",
        "                                           attention_mask=mask,\n",
        "                                           token_type_ids=token_type_ids)\n",
        "\n",
        "        #For every 200-lenght chunk we extracted a representation vector from BERT of size 768 each\n",
        "        #which is the size of the hidden layer\n",
        "        chunks_emb = pooled_out.split_with_sizes(num_chunks)\n",
        "        #print(f'Number of chunks : {num_chunks}')\n",
        "\n",
        "        seq_num_chunkshs = torch.LongTensor([x for x in map(len, chunks_emb)])\n",
        "        #seq_len = tensor(5) for 5 chunks in a document\n",
        "        del pooled_out\n",
        "        #  number of 200-num_chunksh chunk is not fixed\n",
        "        #Pad a list of variable num_chunksh Tensors with padding_value\n",
        "        #sequences (list[Tensor]) – list of variable num_chunksh sequences.\n",
        "        #batch_first (bool, optional) – output will be in B x T x * if True, or in T x B x * otherwise\n",
        "        #padding_value (float, optional) – value for padded elements. Default: 0.\n",
        "        #* is any number of trailing dimensions\n",
        "        '''\n",
        "        , batch size more than one,padding to the max num_chunksh and masking, \n",
        "        in this way we pad the shorter sequences with a special value to be masked (skipped for the network) later.\n",
        "        In this case the special values is -99\n",
        "        '''\n",
        "        # batch_emb_pad = [1,5,768]\n",
        "        batch_emb_pad = nn.utils.rnn.pad_sequence(chunks_emb,\n",
        "                                                  padding_value=-99,\n",
        "                                                  batch_first=True)\n",
        "        # batch_emb = [5,1,768]\n",
        "        batch_emb = batch_emb_pad.transpose(0, 1)  # (B,L,D) -> (L,B,D)\n",
        "\n",
        "        #Packs a Tensor containing padded sequences  from above of variable num_chunksh.\n",
        "        # because each document can be of different len\n",
        "        lstm_input = nn.utils.rnn.pack_padded_sequence(batch_emb, seq_num_chunkshs.cpu().numpy(),\n",
        "                                                       batch_first=False, enforce_sorted=False)\n",
        "        # lstm_input Bactch size is [1,1,1,1,1] meaning 5\n",
        "        #this will reduce it to 100 layers  as initiatized\n",
        "        #packed_output = 100*5() 5 is batch\n",
        "        #h_t = 100 is a  output vector of output shape [1,1,100]\n",
        "        packed_output, (h_t, h_c) = self.lstm(lstm_input, )  # (h_t, h_c))\n",
        "\n",
        "        h_t = h_t.view(-1, 100)\n",
        "\n",
        "        return self.out(h_t)\n",
        "\n",
        "# Train and Evaluate - Bert Model\n",
        "def train_eval_bert_model(data_loader, model, optimizer, device, mode, scheduler=None):\n",
        "    if mode=='train':\n",
        "        model.train()\n",
        "        t0 = time.time()\n",
        "    elif mode=='eval':\n",
        "        model.eval()\n",
        "        target_res = []\n",
        "        output_res = []\n",
        "    else:\n",
        "        print('wrong mode given')\n",
        "\n",
        "    losses = []\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "\n",
        "        # taking out the data from dataloader\n",
        "        ids = [data[\"ids\"] for data in batch]\n",
        "        # This will give list of tensor ids,masks....\n",
        "        mask = [data[\"mask\"] for data in batch]\n",
        "        token_type_ids = [data[\"token_type_ids\"] for data in batch]\n",
        "        targets = [data[\"targets\"] for data in batch]\n",
        "        # [tensor([1, 1, 1], dtype=torch.int32)]<class 'list'>\n",
        "        num_chunks = [data['len'] for data in batch]\n",
        "        #\toutput is [tensor([3])]<class 'list'>\n",
        "\n",
        "        # Converting list into tensors\n",
        "        ids = torch.cat(ids)\n",
        "        mask = torch.cat(mask)\n",
        "        token_type_ids = torch.cat(token_type_ids)\n",
        "        targets = torch.cat(targets)\n",
        "        # tensor([1, 1, 1], dtype=torch.int32)<class 'torch.Tensor'>\n",
        "        num_chunks = torch.cat(num_chunks)\n",
        "        # output is tensor([3])<class 'torch.Tensor'>\n",
        "\n",
        "        # Loading variables to Memory\n",
        "        ids = ids.to(device, dtype=torch.long)\n",
        "        mask = mask.to(device, dtype=torch.long)\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        targets = targets.to(device, dtype=torch.long)\n",
        "\n",
        "        if mode=='train':\n",
        "            # changing all the previous gradients to zero if any\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
        "\n",
        "            # Deleting unused variables which consume quite a bit of memory\n",
        "            del ids, mask, token_type_ids, num_chunks\n",
        "\n",
        "            loss = loss_fun(outputs, targets)\n",
        "            loss.backward()\n",
        "            model.float()\n",
        "            optimizer.step()\n",
        "            if scheduler:\n",
        "                scheduler.step()\n",
        "            losses.append(loss.item())\n",
        "            del loss\n",
        "            if batch_idx % 1000 == 0:\n",
        "                print(\n",
        "                    f\"batch index = {batch_idx} / {len(data_loader)} ({100 * batch_idx / len(data_loader):.2f}%), loss = {np.mean(losses[-10:]):.4f}, time = {(time.time()-t0)//60} minutes \")\n",
        "                t0 = time.time()\n",
        "            output = losses\n",
        "\n",
        "        elif mode =='eval':\n",
        "            with torch.no_grad():\n",
        "                outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
        "                del ids, mask, token_type_ids, num_chunks\n",
        "                loss = loss_fun(outputs, targets)\n",
        "                losses.append(loss.item())\n",
        "                del loss\n",
        "\n",
        "            target_res.append(targets.cpu().detach().numpy())\n",
        "            output_res.append(torch.softmax(\n",
        "                outputs, dim=1).cpu().detach().numpy())\n",
        "            output = np.concatenate(output_res), np.concatenate(target_res), losses\n",
        "\n",
        "    return  output\n",
        "\n",
        "# Train and Evaluate - RoBert Model\n",
        "def train_eval_robert_model(data_loader, model, optimizer, device, mode, scheduler=None):\n",
        "    if mode == 'train':\n",
        "        model.train()\n",
        "        t0 = time.time()\n",
        "    elif mode == 'eval':\n",
        "        model.eval()\n",
        "        target_res = []\n",
        "        output_res = []\n",
        "    else:\n",
        "        print('wrong mode given')\n",
        "\n",
        "    losses = []\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "\n",
        "        # taking out the data from dataloader\n",
        "        ids = [data[\"ids\"] for data in batch]\n",
        "        mask = [data[\"mask\"] for data in batch]\n",
        "        token_type_ids = [data[\"token_type_ids\"] for data in batch]\n",
        "        targets = [data[\"targets\"][0] for data in batch]\n",
        "        num_chunks = [data['len'] for data in batch]\n",
        "        # input is <class 'list'>\n",
        "        # targets and its type : [tensor(0, dtype=torch.int32)]<class 'list'>\n",
        "        # num_chunks and its type : [tensor([5])]<class 'list'>\n",
        "        ids = torch.cat(ids)\n",
        "        mask = torch.cat(mask)\n",
        "        token_type_ids = torch.cat(token_type_ids)\n",
        "        targets = torch.stack(targets)\n",
        "        num_chunks = torch.cat(num_chunks)\n",
        "        num_chunks = [x.item() for x in num_chunks]\n",
        "        # stacked targets and its type : tensor([0], dtype=torch.int32)<class 'torch.Tensor'>\n",
        "        # concatenated num_chunks and its type : tensor([5])<class 'torch.Tensor'>\n",
        "        # ietms num_chunks and its type : [5]<class 'list'>\n",
        "        ids = ids.to(device, dtype=torch.long)\n",
        "        mask = mask.to(device, dtype=torch.long)\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        targets = targets.to(device, dtype=torch.long)\n",
        "\n",
        "        if mode == 'train':\n",
        "            # changing all the previous gradients to zero if any\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(ids=ids, mask=mask,\n",
        "                            token_type_ids=token_type_ids, num_chunks=num_chunks)\n",
        "            del ids, mask, token_type_ids, num_chunks\n",
        "            loss = loss_fun(outputs, targets)\n",
        "            loss.backward()\n",
        "            model.float()\n",
        "            optimizer.step()\n",
        "            if scheduler:\n",
        "                scheduler.step()\n",
        "            losses.append(loss.item())\n",
        "            del loss\n",
        "            if batch_idx % 1000 == 0:\n",
        "              print(\n",
        "                f\"batch index = {batch_idx} / {len(data_loader)} ({100*batch_idx / len(data_loader):.2f}%), loss = {np.mean(losses[-10:]):.4f}, time = {(time.time()-t0)//60} minutes\")\n",
        "              t0 = time.time()\n",
        "            output = losses\n",
        "\n",
        "        elif mode == 'eval':\n",
        "            with torch.no_grad():\n",
        "                outputs = model(ids=ids, mask=mask,\n",
        "                                token_type_ids=token_type_ids, num_chunks=num_chunks)\n",
        "                loss = loss_fun(outputs, targets)\n",
        "                losses.append(loss.item())\n",
        "            target_res.append(targets.cpu().detach().numpy())\n",
        "            output_res.append(torch.softmax(\n",
        "                outputs, dim=1).cpu().detach().numpy())\n",
        "            output = np.concatenate(output_res), np.concatenate(target_res), losses\n",
        "\n",
        "    return output\n",
        "\n",
        "# To create a dataloader with variable-size input\n",
        "def my_collate1(batches):\n",
        "    return [{key: torch.stack(value) for key, value in batch.items()} for batch in batches]\n",
        "\n",
        "# Loss function\n",
        "def loss_fun(outputs, targets):\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    return loss(outputs, targets)\n",
        "    # return nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "# For predictions and accuracy\n",
        "def evaluate(target, predicted):\n",
        "    true_label_mask = [1 if (np.argmax(x) - target[i]) ==\n",
        "                            0 else 0 for i, x in enumerate(predicted)]\n",
        "    nb_prediction = len(true_label_mask)\n",
        "    true_prediction = sum(true_label_mask)\n",
        "    false_prediction = nb_prediction - true_prediction\n",
        "    Accuracy = true_prediction / nb_prediction\n",
        "    return {\n",
        "        \"Accuracy\": Accuracy,\n",
        "        \"No. of examples\": len(target),\n",
        "        \"True prediction\": true_prediction,\n",
        "        \"False prediction\": false_prediction,\n",
        "    }\n",
        "#----------------------------------------------\n",
        "# can join evaluate and f1 score and remove accuracy \n",
        "\n",
        "# Precision () is defined as the number of true positives () over \n",
        "# the number of true positives plus the number of false positives \n",
        "def get_f1_precision_recall(y_test,y_pred):\n",
        "    \n",
        "  float_formatter = \"{:.2f}\".format\n",
        "  np.set_printoptions(formatter={'float_kind':float_formatter})\n",
        "\n",
        "  precision, recall, fscore, support = score(y_test, y_pred)\n",
        "  print('precision: {}'.format(precision))\n",
        "  print('recall   : {}'.format(recall))\n",
        "  print('fscore   : {}'.format(fscore))\n",
        "  return\n",
        "\n",
        "# For plotting confusion matrix\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    import itertools\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "# To create, clean and divide the dataset into chunks\n",
        "class FakeNewsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, max_len, chunk_len=200, overlap_len=50, max_size_dataset=None,\n",
        "                  file_location=file_location, min_len=10, mode='train_eval', testdf=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.overlap_len = overlap_len\n",
        "        self.chunk_len = chunk_len\n",
        "        self.min_len = min_len\n",
        "        self.max_size_dataset = max_size_dataset\n",
        "        self.testdf = testdf\n",
        "        self.mode = mode\n",
        "        self.data, self.label,self.label_count,self.label_mapping= self.process_data(file_location,mode,testdf )\n",
        "\n",
        "    def process_data(self, file_location,mode,testdf):\n",
        "        \n",
        "        base_l = []\n",
        "        # dataset for training and evaluation\n",
        "        if mode == 'train_eval':\n",
        "          df = pd.read_csv(file_location)\n",
        "          train_raw = df[df.body.notnull()]\n",
        "          train_raw = train_raw[train_raw.title.notnull()]\n",
        "        # dataset for predicting the single outcome\n",
        "        elif mode == 'test':\n",
        "          train_raw = testdf.copy()\n",
        "\n",
        "        \n",
        "        base_l = get_base_url (train_raw.url)\n",
        "        train_raw['base_url'] = base_l\n",
        "        train_raw = train_raw.assign(len_txt=train_raw.body.apply(lambda x: len(x.split())))\n",
        "        train_raw = train_raw[train_raw.len_txt > self.min_len]\n",
        "\n",
        "        train_raw = train_raw[['title','body', 'label','base_url']]\n",
        "        train_raw.reset_index(inplace=True, drop=True)\n",
        "        label_count = train_raw['label'].value_counts()\n",
        "        LE = LabelEncoder()\n",
        "        train_raw['label'] = LE.fit_transform(train_raw['label'])\n",
        "        label_mapping = dict(zip(LE.classes_, range(len(LE.classes_))))\n",
        "        \n",
        "        train = train_raw.copy()\n",
        "        del train_raw\n",
        "        if (self.max_size_dataset):\n",
        "            train = train.loc[0:self.max_size_dataset, :]\n",
        "        train = train.reindex(np.random.permutation(train.index))\n",
        "      \n",
        "        train['body'] = train.body.apply(self.clean_txt)\n",
        "        train['title'] = train.title.apply(self.clean_txt)\n",
        "        train['text'] = train[['title','body', 'base_url']].agg(' '.join, axis=1)\n",
        "        return train['text'].values, train['label'].values,label_count,label_mapping\n",
        "\n",
        "    def clean_txt(self, text):\n",
        "        # Apply regex and clean the data\n",
        "        text = re.sub(\"'\", \"\", text)\n",
        "        text = re.sub(\"(\\\\W)+\", \" \", text)\n",
        "        text = re.sub(r'[^A-Z-a-z. \\d/,]', '', text.lower())\n",
        "        text = ' '.join(text.split())\n",
        "        return text\n",
        "\n",
        "    def long_terms_tokenizer(self, data_tokenize, targets):\n",
        "        \n",
        "        input_ids_list = []\n",
        "        attention_mask_list = []\n",
        "        token_type_ids_list = []\n",
        "        targets_list = []\n",
        "\n",
        "        previous_input_ids = data_tokenize[\"input_ids\"].reshape(-1)\n",
        "        previous_attention_mask = data_tokenize[\"attention_mask\"].reshape(-1)\n",
        "        previous_token_type_ids = data_tokenize[\"token_type_ids\"].reshape(-1)\n",
        "\n",
        "        # it contains tokens that are after 200 num_chunksh because of CHUNK_LEN=200\n",
        "        overflowing_ids = data_tokenize.get(\"overflowing_tokens\").reshape(-1)  # added .reshape(-1)\n",
        "        targets = torch.tensor(targets, dtype=torch.int)\n",
        "\n",
        "        input_ids_list.append(previous_input_ids)\n",
        "        attention_mask_list.append(previous_attention_mask)\n",
        "        token_type_ids_list.append(previous_token_type_ids)\n",
        "        targets_list.append(targets)\n",
        "\n",
        "        if overflowing_ids.nelement() != 0 :\n",
        "            overflowing_ids = torch.tensor(overflowing_ids, dtype=torch.long)\n",
        "            # total no. of index, chunck + overflow\n",
        "            # range means how many steps of 200 chunks eg if len is 723, then range is 4\n",
        "            idxs = range(len(overflowing_ids) + self.chunk_len)\n",
        "            # from where the next chunk starts\n",
        "            # idxs = start, step, stop, 148,148,396\n",
        "            idxs = idxs[(self.chunk_len - self.overlap_len - 2)\n",
        "                        ::(self.chunk_len - self.overlap_len - 2)]\n",
        "            # input_ids_first_overlap last 50 tokens of the first 200 batch\n",
        "            input_ids_first_overlap = previous_input_ids[-(\n",
        "                    self.overlap_len + 1):-1]\n",
        "            start_token = torch.tensor([101], dtype=torch.long)\n",
        "            end_token = torch.tensor([102], dtype=torch.long)\n",
        "\n",
        "            for i, idx in enumerate(idxs):\n",
        "                # enumerate i = counter, 1,2 3--\n",
        "                if i == 0:\n",
        "                    # input_ids = 50 from last of the first batch + 148 from overflowing_ids + 2 tokens\n",
        "                    input_ids = torch.cat(\n",
        "                        (input_ids_first_overlap, overflowing_ids[:idx]))\n",
        "                elif i == len(idxs):\n",
        "                    # for the last overflowing_idsing indexes\n",
        "                    input_ids = overflowing_ids[idx:]\n",
        "                elif previous_idx >= len(overflowing_ids):\n",
        "                    # when the indexes finishes it breaks\n",
        "                    break\n",
        "                else:\n",
        "                    # if not the first and the last, in between then last index-50, to overlap till next 150\n",
        "                    input_ids = overflowing_ids[(previous_idx - self.overlap_len):idx]\n",
        "\n",
        "                previous_idx = idx\n",
        "\n",
        "                # after getting input ids add equivalant amount of attention mask and then token ids\n",
        "                nb_token = len(input_ids) + 2\n",
        "                attention_mask = torch.ones(self.chunk_len, dtype=torch.long)  # chunk len = 200\n",
        "                attention_mask[nb_token:self.chunk_len] = 0\n",
        "                token_type_ids = torch.zeros(self.chunk_len, dtype=torch.long)\n",
        "                input_ids = torch.cat((start_token, input_ids, end_token))\n",
        "                # if its input id is less then 200, then add padding till 200\n",
        "                if self.chunk_len - nb_token > 0:\n",
        "                    padding = torch.zeros(\n",
        "                        self.chunk_len - nb_token, dtype=torch.long)\n",
        "                    input_ids = torch.cat((input_ids, padding))\n",
        "\n",
        "                input_ids_list.append(input_ids)\n",
        "                attention_mask_list.append(attention_mask)\n",
        "                token_type_ids_list.append(token_type_ids)\n",
        "                targets_list.append(targets)\n",
        "        return ({\n",
        "            'ids': input_ids_list,  # torch.tensor(ids, dtype=torch.long),\n",
        "            # torch.tensor(mask, dtype=torch.long),\n",
        "            'mask': attention_mask_list,\n",
        "            # torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'token_type_ids': token_type_ids_list,\n",
        "            'targets': targets_list,\n",
        "            'len': [torch.tensor(len(targets_list), dtype=torch.long)]\n",
        "        })\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        body = str(self.data[idx])\n",
        "        targets = int(self.label[idx])\n",
        "        data = self.tokenizer.encode_plus(\n",
        "            body,\n",
        "            truncation=True,\n",
        "            max_length=self.chunk_len,\n",
        "            padding='longest',\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            return_overflowing_tokens=True,\n",
        "            return_tensors='pt')\n",
        "        # calling third function\n",
        "        long_token = self.long_terms_tokenizer(data, targets)\n",
        "        return long_token\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" Return data length \"\"\"\n",
        "        return self.label.shape[0]\n",
        "\n",
        "# For extracting base url from complete url\n",
        "def get_base_url (long_url):  \n",
        "  ls = []\n",
        "  for item in long_url:\n",
        "      u=unquote(item)\n",
        "      g=urlparse(u)\n",
        "      base_url=g.netloc\n",
        "      ls.append(base_url)\n",
        "  return ls\n",
        "\n",
        "# For extracting article from the URL \n",
        "def get_text_from_url(url):\n",
        "  try:\n",
        "    extractor = extractors.ArticleExtractor()\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'\n",
        "    }\n",
        "    resp = requests.get(url, headers=headers)\n",
        "    if resp.ok:\n",
        "        doc = extractor.get_content(resp.text)\n",
        "    else:\n",
        "        doc = get_body_from_soup(url)\n",
        "    testarticle = pd.DataFrame(columns=['title','body','label','url']) #header=None\n",
        "    title = 'news'\n",
        "    body = doc\n",
        "    label = 'fake'\n",
        "    url = url\n",
        "    testarticle = testarticle.append({'title': title, 'body': body, 'label': label,'url':url}, ignore_index=True)\n",
        "  except:\n",
        "    print(\"Please use different URL\")\n",
        "  return testarticle\n",
        "\n",
        "#if the above function fails, then use beautiful soup, just a check safe(its not used tho)\n",
        "def get_body_from_soup(url):\n",
        "  res = requests.get(url)\n",
        "  html_page = res.content\n",
        "  soup = BeautifulSoup(html_page, 'html.parser')\n",
        "  text = soup.find_all(text=True)\n",
        "  set([t.parent.name for t in text])\n",
        "  output = ''\n",
        "  want = ['body','title','p','div','article','h1','h2','h3','h4']\n",
        "  for t in text:\n",
        "      if t.parent.name in want:\n",
        "          output += '{} '.format(t)\n",
        "  return output\n",
        "\n",
        "def predict_article(url,model,model_roberta):\n",
        "    \n",
        "\n",
        "  url = url\n",
        "  testarticle = get_text_from_url(url)\n",
        "\n",
        "  # check if article is related to Covid-19 or not\n",
        "  cov_rel_words = 'covid|corona|covid-19|covid19|coronavirus|pandemic|crisis|disease|quarantine|distancing|immunity|symptoms|viruses|epidemic|lockdown|vaccine'\n",
        "  related = testarticle['body'].str.lower().str.contains(cov_rel_words)\n",
        "  if related.bool() :\n",
        "    # Extracting Dataset\n",
        "    testdataset = FakeNewsDataset(\n",
        "                              tokenizer=bert_tokenizer,\n",
        "                              min_len=MIN_LEN,\n",
        "                              max_len=MAX_LEN,\n",
        "                              chunk_len=CHUNK_LEN,\n",
        "                              max_size_dataset=1,\n",
        "                              overlap_len=OVERLAP_LEN,\n",
        "                              mode='test',\n",
        "                              testdf=testarticle\n",
        "                              )\n",
        "\n",
        "    test_data_loader=DataLoader(testdataset,\n",
        "                                  batch_size=1,\n",
        "                                  sampler=None,\n",
        "                                  collate_fn=my_collate1)\n",
        "    num_training_steps = int(len(testdataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
        "    # Initializing/ loading Bert model trained before\n",
        "    model = model\n",
        "\n",
        "    #Loading all the layers of the bert model except the last one as explained in the class above.\n",
        "    model_roberta = model_roberta\n",
        "    optimizer = AdamW(model_roberta.parameters(), lr=lr)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0,\n",
        "                                                num_training_steps=num_training_steps)\n",
        "    val_losses = []\n",
        "    output, target, val_losses_tmp = train_eval_robert_model(test_data_loader, model_roberta, optimizer, device,'eval')\n",
        "    y_pred = np.argmax(output, axis=1).flatten()[0] #Estimated targets as returned by a classifier.\n",
        "    pred_label = dict(map(reversed, dataset.label_mapping.items()))\n",
        "    \n",
        "    return pred_label[y_pred]\n",
        "  \n",
        "  else:\n",
        "    msg = 'Please enter url related to Coronavirus'\n",
        "    return msg\n",
        "\n",
        "\n",
        "# Check if Graphic card is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "## HyperParameters\n",
        "TRAIN_BATCH_SIZE = 1\n",
        "EPOCH = 2\n",
        "DATASET_SPLIT = .25\n",
        "shuffle_dataset = True\n",
        "random_seed = 42\n",
        "# MIN_LEN=249\n",
        "MIN_LEN = 10\n",
        "MAX_LEN = 50\n",
        "CHUNK_LEN = 200\n",
        "OVERLAP_LEN = 50\n",
        "max_size_dataset =None\n",
        "NUM_CLASSES = 4\n",
        "lr = 1e-5\n",
        "# Defining tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "# Extracting Dataset\n",
        "dataset = FakeNewsDataset(\n",
        "                          tokenizer=bert_tokenizer,\n",
        "                          min_len=MIN_LEN,\n",
        "                          max_len=MAX_LEN,\n",
        "                          chunk_len=CHUNK_LEN,\n",
        "                          max_size_dataset=max_size_dataset,\n",
        "                          overlap_len=OVERLAP_LEN,\n",
        "                          mode='train_eval',\n",
        "                          testdf=None\n",
        "                          )\n",
        "\n",
        "dataset_size = len(dataset)\n",
        "print('Total articles :' + str(dataset_size))\n",
        "\n",
        "# Because the labels are imbalanced, we split the data set in a stratified fashion, using this as the class labels.\n",
        "#Dividing into test and Validating\n",
        "train_idx, valid_idx = train_test_split(\n",
        "                                        np.arange(dataset_size),\n",
        "                                        test_size=DATASET_SPLIT,\n",
        "                                        shuffle=True,\n",
        "                                        stratify=dataset.label)\n",
        "\n",
        "# Creating data indices for training and validation splits:\n",
        "#Map-style datasets\n",
        "#A map-style dataset is one that implements the __getitem__() and __len__() protocols,\n",
        "# and represents a map from (possibly non-integral) indices/keys to data samples.\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_data_loader=DataLoader(\n",
        "                              dataset,\n",
        "                              batch_size=TRAIN_BATCH_SIZE,\n",
        "                              sampler=train_sampler,\n",
        "                              collate_fn=my_collate1)\n",
        "\n",
        "valid_data_loader=DataLoader(\n",
        "                              dataset,\n",
        "                              batch_size=TRAIN_BATCH_SIZE,\n",
        "                              sampler=valid_sampler,\n",
        "                              collate_fn=my_collate1)\n",
        "## Load Trained Model\n",
        "'''model = Bert_TextClassification_Model()\n",
        "model.load_state_dict(torch.load('/content/Gdrive/MyDrive/finetuned_BERT_Model-2020-12-14-2.pt'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "model_robert = RoBERT_Model(bertFineTuned=list(model.children())[0]).to(device)\n",
        "model_robert.load_state_dict(torch.load('/content/Gdrive/MyDrive/RoBERT_Model-2020-12-14-2.pt'))\n",
        "model_robert.to(device)\n",
        "model_robert.eval()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting lstm_bert_model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BntckG4zzawO"
      },
      "source": [
        "## User Interface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhF0kqqOTkev"
      },
      "source": [
        "!pip -q install streamlit\n",
        "!pip -q install pyngrok\n",
        "!cp '/content/Gdrive/MyDrive/style.css' .\n",
        "!cp -r '/content/Gdrive/MyDrive/fake_news' ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM3SWbuEToCK",
        "outputId": "2a0dfb23-e6b8-4c2c-99d8-92f6d0b6512f"
      },
      "source": [
        "%%writefile app.py\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/Gdrive\")\n",
        "import streamlit as st\n",
        "import torch\n",
        "from lstm_bert_model import Bert_TextClassification_Model, RoBERT_Model, predict_article\n",
        "from PIL import Image \n",
        "import validators\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "st.set_option('deprecation.showfileUploaderEncoding',False)\n",
        "st.title(\"Fake News Detector\")\n",
        "\n",
        "# For loading Stylesheet for the UI\n",
        "def local_css(file_name):\n",
        "    with open(file_name) as f:\n",
        "        st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)\n",
        "\n",
        "def remote_css(url):\n",
        "    st.markdown(f'<link href=\"{url}\" rel=\"stylesheet\">', unsafe_allow_html=True)    \n",
        "# For loading the saved model\n",
        "@st.cache()\n",
        "def load_model():\n",
        "  model = Bert_TextClassification_Model()\n",
        "  model.load_state_dict(torch.load('/content/Gdrive/MyDrive/finetuned_BERT_Model-2021-03-03-1.pt'))\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  model_robert = RoBERT_Model(bertFineTuned=list(model.children())[0]).to(device)\n",
        "  model_robert.load_state_dict(torch.load('/content/Gdrive/MyDrive/RoBERT_Model-2021-03-04-2.pt'))\n",
        "  model_robert.to(device)\n",
        "  model_robert.eval()\n",
        "  return model, model_robert\n",
        "\n",
        "with st.spinner('Loading Model Into Memory......'):\n",
        "  model, model_robert = load_model()\n",
        "local_css(\"style.css\")\n",
        "remote_css('https://fonts.googleapis.com/icon?family=Material+Icons')\n",
        "\n",
        "#labels = {'High probability fake ': 0, 'High probability real': 1, 'fake': 2, 'real': 3}\n",
        "\n",
        "High_prob_fake_img = '/content/fake_news/High_prob_fake.jpg'\n",
        "High_prob_real_img = '/content/fake_news/High_prob_real.jpg'\n",
        "fake_img = '/content/fake_news/fake.jpg'\n",
        "real_img = '/content/fake_news/real.jpg'\n",
        "\n",
        "\n",
        "st.text('Please enter the URL')\n",
        "user_input_url = st.text_input(\"\")\n",
        "button_clicked = st.button(\"OK\")\n",
        "if button_clicked:\n",
        "  # Check if its a valid url\n",
        "  if validators.url(user_input_url):\n",
        "    # Extract the article into text and predict the label\n",
        "    result = predict_article(user_input_url,model=model,model_roberta=model_robert)\n",
        "    if result == 'fake':\n",
        "      st.image(fake_img,width=None)\n",
        "    elif result =='real':\n",
        "      st.image(real_img,width=None)\n",
        "    elif result =='High probability fake ':\n",
        "      st.image(High_prob_fake_img,width=None)\n",
        "    elif result =='High probability real':\n",
        "      st.image(High_prob_real_img,width=None)\n",
        "    else:\n",
        "      # If the url is valid but the information is not relevant\n",
        "      st.write('Could not find any relevant information related to Covid-19. Please try with a different URL')\n",
        "  else:\n",
        "    # If invalid url\n",
        "    st.write('Please enter valid URL')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbAB9mBsTs5q",
        "outputId": "aecec4d6-42c2-425e-c5d3-dba86ac6080d"
      },
      "source": [
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(port='80')\n",
        "print (public_url)\n",
        "!streamlit run --server.port 80 app.py >/dev/null"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NgrokTunnel: \"http://b2f12056bf86.ngrok.io\" -> \"http://localhost:80\"\n",
            "2021-03-31 11:51:29.568 An update to the [server] config option section was detected. To have these changes be reflected, please restart streamlit.\n",
            "2021-03-31 11:51:30.733935: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-03-31 11:51:35.769 NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbfDa-t-BZ_a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gcueSuCH3O-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}